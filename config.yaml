# Network parameters configuration

# Models to run True False 
run_DQlearning: False 
run_surr_opt: True 
run_adaptive_contrast_tao: False 
setting: 'new' # 'linear', 'tao', 'scheme_5', 'scheme_6', 'scheme_7', 'scheme_8', 'new' 
num_replications: 3 # 30, 100
sample_size: 15000 # 15000, 30000, 10000  # Number of samples to be used 
batch_size: 800  #700, 300  # Batch size calculated as a proportion of sample size

n_epoch: 60 # 150  # Number of training epochs
num_layers: 4
optimizer_type: 'adam'  # Optimizer type, can be 'adam' or 'rmsprop'
optimizer_lr: 0.07  # Learning rate for the optimizer
dropout_rate: 0.4  # Dropout rate to prevent overfitting
activation_function: 'elu' # elu, relu, sigmoid, tanh, leakyrelu, none
hidden_dim_stage1: 40  # Number of neurons in the hidden layer of stage 1
hidden_dim_stage2: 40  # Number of neurons in the hidden layer of stage 2 
gradient_clipping: True # True, False
add_ll_batch_norm: False
phi_ensamble: True # if this is true keep ensemble_count to 5
ensemble_count: 5 # 7 

# # scheme_5 constants 
# C1: 3.0 
# C2: 3.0 
# beta: 1 
# cnst: 1 # 1, 5, 10, 20, 30, 100 

# # scheme_6 constants 
# C1: 5.0 
# C2: 5.0 
# beta: 1 
# cnst: 10 # 10 
# b: 2 

# scheme 8 
C1: 3.0 
C2: 3.0 
neu: 5 # 10 
u: 5 # 10 
alpha: 5 # 10
f: "square" #'square', arctan, sin, exp_half, exp, tan
m1 : 'sin'
m2: 'sin'

# # new scheme
input_dim: 100 #10, 50, 100 # only for new scheme



# f_model: 'model_name'  # DQlearning, surr_opt, tao - igore here
noiseless: False # True, False  # Boolean flag to indicate if noise should be excluded in simulations
use_m_propen: True # True, False
interaction_terms: False

surrogate_num: 1 #1  # Indicates the surrogate model configuration number
option_sur: 2 #4  # Specifies the operational mode or variant of the surrogate model
device: None  # Computation device, dynamically set to 'cuda' if GPU is available
# job_id: tao 

training_validation_prop: 0.7  # Proportion of data for training vs validation
num_networks: 2  # Number of parallel networks or models

#  Input dimension for stage 1, [O1] --> [x1, x2, x3, x4, x5] input_dim_stage1: 5 
#  Input dimension for stage 2, includes [O1, A1, Y1, O2] input_dim_stage2: 7 

output_dim_stage1: 1  # Output dimension for stage 1
output_dim_stage2: 1  # Output dimension for stage 2
optimizer_weight_decay: 0.001  # Weight decay (L2 regularization) helps prevent overfitting

use_scheduler: True # True, False
scheduler_type: 'reducelronplateau'  # Type of learning rate scheduler, can be 'reducelronplateau', 'steplr', or 'cosineannealing'
scheduler_step_size: 30  # Step size for StepLR, defines the number of epochs before the next LR decay
scheduler_gamma: 0.8  # Decay rate for learning rate under StepLR
initializer: 'he'  # He initialization method (Kaiming initialization)

contrast: 1

# # scheme_i constants
# C1: 3.0
# C2: 3.0
# beta: 1
# cnst: 1 


